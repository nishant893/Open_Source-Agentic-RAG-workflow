{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"LLamaParse API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ_API_KEY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    num_workers=1,\n",
    "    premium_mode = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 1/1 [02:22<00:00, 142.54s/it]\n"
     ]
    }
   ],
   "source": [
    "documents = parser.load_data([r\"C:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\iesc111.pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[3].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "nodes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "splitter1 = MarkdownNodeParser(\n",
    "    include_metadata= False,\n",
    "    include_prev_next_rel= True,\n",
    ")\n",
    "\n",
    "nodes_markdown = splitter1.get_nodes_from_documents(documents)\n",
    "\n",
    "nodes_markdown[1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes_markdown_element = node_parser.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexNode(id_='db060258-4878-4705-be5c-386b4978e71f', embedding=None, metadata={'col_schema': 'Column: State\\nType: string\\nSummary: None\\n\\nColumn: Substance\\nType: string\\nSummary: None\\n\\nColumn: Speed in m/s\\nType: number\\nSummary: None'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7a502ce8-cada-4f01-b621-6b8f4ae55c54', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='9d0d77c9411a751597c9f7f9be255d1ae84b92311c71dcf545bdc170e8689b54'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='56f63ebd-6025-4439-8286-993512753375', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d8f6433e378187647f66887e09562443b2f6efd17af31115e71054d502320695'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8ea8291c-cc0c-4de4-a8a9-be49d3a6b694', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' State   ': {0: ' Solids  ', 1: '         ', 2: '         ', 3: '         ', 4: '         ', 5: '         ', 6: ' Liquids ', 7: '         ', 8: '         ', 9: '         ', 10: ' Gases   ', 11: '         ', 12: '         ', 13: '         ', 14: '         '}, ' Substance       ': {0: ' Aluminium       ', 1: ' Nickel          ', 2: ' Steel           ', 3: ' Iron            ', 4: ' Brass           ', 5: ' Glass (Flint)   ', 6: ' Water (Sea)     ', 7: ' Water (distilled)', 8: ' Ethanol         ', 9: ' Methanol        ', 10: ' Hydrogen        ', 11: ' Helium          ', 12: ' Air             ', 13: ' Oxygen          ', 14: ' Sulphur dioxide '}, ' Speed in m/s ': {0: 6420, 1: 6040, 2: 5960, 3: 5950, 4: 4700, 5: 3980, 6: 1531, 7: 1498, 8: 1207, 9: 1103, 10: 1284, 11: 965, 12: 346, 13: 316, 14: 213}}\", 'table_summary': 'Speed of sound in different substances,\\nwith the following table title:\\nSpeed of sound in different substances,\\nwith the following columns:\\n- State: None\\n- Substance: None\\n- Speed in m/s: None\\n'}, hash='4fbbe60bcc87e8f64bf44087080e1099764bc1a1c75d9aeaab037b41c456f2cc')}, text='Speed of sound in different substances,\\nwith the following table title:\\nSpeed of sound in different substances,\\nwith the following columns:\\n- State: None\\n- Substance: None\\n- Speed in m/s: None\\n', mimetype='text/plain', start_char_idx=969, end_char_idx=1734, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='8ea8291c-cc0c-4de4-a8a9-be49d3a6b694', obj=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_markdown_element[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes_markdown_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_markdown_element = VectorStoreIndex(nodes=base_nodes+objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-postprocessor-flag-embedding-reranker git+https://github.com/FlagOpen/FlagEmbedding.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=3,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_element_query_engine = index_markdown_element.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_parser():\n",
    "    return LlamaParse(\n",
    "        result_type=\"markdown\",\n",
    "        verbose=True,\n",
    "        language=\"en\",\n",
    "        num_workers=1,\n",
    "        premium_mode=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_index(embed_model, db_path=\"./chroma_db_new\"):\n",
    "    parser = setup_parser()\n",
    "    documents = parser.load_data([r\"C:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\iesc111.pdf\"])\n",
    "    \n",
    "    node_parser = MarkdownElementNodeParser(llm=Settings.llm, num_workers=8)\n",
    "    nodes_markdown_element = node_parser.get_nodes_from_documents(documents=documents)\n",
    "    base_nodes, objects = node_parser.get_nodes_and_objects(nodes_markdown_element)\n",
    "    \n",
    "    # Create or connect to the persistent ChromaDB\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "    # Create ChromaVectorStore\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # Create storage context\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # Create index\n",
    "    index = VectorStoreIndex(nodes=base_nodes+objects, storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 1/1 [00:17<00:00, 17.39s/it]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x2318b167490>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_and_save_index(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "2024-09-30 11:40:10,526 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance of the reflecting surface from the source of sound production remains the same. However, the speed of sound increases with an increase in temperature. Therefore, the time taken for the sound to travel to the reflecting surface and back will decrease on a hotter day. Since the time interval between the original sound and the reflected one must be at least 0.1 s to hear a distinct echo, the echo sound may not be heard on a hotter day if the time interval becomes less than 0.1 s.\n"
     ]
    }
   ],
   "source": [
    "eq = \"\"\"When  a  sound  is  reflected  from  a  distant  object,  an  echo  is\n",
    "produced.  Let  the  distance  between  the  reflecting  surface\n",
    "and  the  source  of  sound  production  remains  the  same.  Do\n",
    "you hear echo sound on a hotter day?\"\"\"\n",
    "\n",
    "response_element1 = markdown_element_query_engine.query(eq)\n",
    "print(response_element1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq2 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "response_element2 = markdown_element_query_engine.query(eq2)\n",
    "print(response_element2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowState:\n",
    "    def __init__(self, query: str, initial_response: str):\n",
    "        self.query = query\n",
    "        self.initial_response = initial_response\n",
    "        self.additional_info = \"\"\n",
    "        self.final_answer = \"\"\n",
    "        self.needs_info = False\n",
    "\n",
    "\n",
    "def check_completeness(state: WorkflowState, llm: Groq) -> WorkflowState:\n",
    "    prompt = f\"\"\"\n",
    "    Given the following query and initial response, determine if the response contains \n",
    "    all necessary information to fully answer the query. If not, specify what additional \n",
    "    information is needed.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "\n",
    "    Output your response in the following format:\n",
    "    Complete: [Yes/No]\n",
    "    Additional Info Needed: [If No, specify what additional information is needed]\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    # Extract 'Complete' and 'Additional Info Needed' fields from response\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'Complete' in line:\n",
    "            state.needs_info = 'No' in line\n",
    "        elif 'Additional Info Needed' in line and state.needs_info:\n",
    "            state.additional_info = line.split(': ')[1].strip()\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def retrieve_additional_info(state: WorkflowState, query_engine: RetrieverQueryEngine) -> WorkflowState:\n",
    "    if state.needs_info:\n",
    "        additional_info_query = state.additional_info\n",
    "        response = query_engine.query(additional_info_query)\n",
    "\n",
    "        if isinstance(response, str):\n",
    "            state.additional_info = response.strip()\n",
    "        else:\n",
    "            raise TypeError(\"Expected string response from query engine.\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_final_answer(state: WorkflowState, llm: Groq) -> WorkflowState:\n",
    "    prompt = f\"\"\"\n",
    "    Given the following query, initial response, and additional information (if any),\n",
    "    generate a complete and accurate answer.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "    Additional Information: {state.additional_info}\n",
    "\n",
    "    Complete Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        state.final_answer = response.strip()\n",
    "    else:\n",
    "        raise TypeError(\"Expected string response from generate_final_answer.\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def create_workflow(query_engine: RetrieverQueryEngine, llm: Groq):\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: check_completeness(x, llm),\n",
    "            name=\"check_completeness\",\n",
    "            description=\"Check if the initial response is complete\"\n",
    "        ),\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: retrieve_additional_info(x, query_engine),\n",
    "            name=\"retrieve_additional_info\",\n",
    "            description=\"Retrieve additional information if needed\"\n",
    "        ),\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: generate_final_answer(x, llm),\n",
    "            name=\"generate_final_answer\",\n",
    "            description=\"Generate the final answer\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=llm,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def solve_query_with_workflow(query: str, query_engine: RetrieverQueryEngine, llm: Groq) -> str:\n",
    "    # Get initial response\n",
    "    initial_response = query_engine.query(query)\n",
    "\n",
    "    # Create initial state\n",
    "    state = WorkflowState(query=query, initial_response=str(initial_response))\n",
    "\n",
    "    # Create and run the workflow\n",
    "    agent = create_workflow(query_engine, llm)\n",
    "\n",
    "    # Define the task for the agent\n",
    "    task = f\"\"\"\n",
    "    Given the following query and initial response, use the available tools to ensure a complete and accurate answer is provided.\n",
    "    If the initial response is incomplete, retrieve additional information and generate a final answer.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "\n",
    "    Use the tools in this order:\n",
    "    1. check_completeness\n",
    "    2. retrieve_additional_info (if needed)\n",
    "    3. generate_final_answer\n",
    "\n",
    "    Return the final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the agent\n",
    "    response = agent.chat(task)\n",
    "\n",
    "    # Process the response to extract the final answer\n",
    "    if hasattr(response, 'response'):\n",
    "        final_answer = response.response.strip()\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected response format from agent.\")\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "#logger.info(f\"Processing query: {query}\")\n",
    "final_answer = solve_query_with_workflow(query, markdown_element_query_engine, llm)\n",
    "#logger.info(f\"Final answer: {final_answer}\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq2 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "final_answer = solve_query_with_workflow(eq2, markdown_element_query_engine, llm)\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    def __init__(self, query_engine: BaseQueryEngine):\n",
    "        self.query_engine = query_engine\n",
    "\n",
    "    def process_query(self, query: str) -> Dict[str, str]:\n",
    "        response = self.query_engine.query(query)\n",
    "        return {\n",
    "            \"response\": response.response,\n",
    "            \"source_nodes\": [node.node.get_content() for node in response.source_nodes],\n",
    "        }\n",
    "\n",
    "class InformationAnalyzer:\n",
    "    def __init__(self, groq_llm):\n",
    "        self.llm = groq_llm\n",
    "\n",
    "    def analyze_response(self, initial_response: str, query: str) -> str:\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"\"\"You are an AI assistant tasked with analyzing responses and determining if additional information is needed. \n",
    "            Request additional information if:\n",
    "            1. The query cannot be fully answered without additional specific data.\n",
    "            2. The query asks for a specific numerical value, and the initial response doesn't provide it.\n",
    "            3. The initial response uses variables or placeholders without providing their actual values.\n",
    "            If additional information is needed, specify it as a simple, direct question.\"\"\"),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"\n",
    "                Given the following query and initial response, determine if any additional information is required to fully answer the query. \n",
    "                If the initial response fully answers the query with all necessary specific values, state that no additional information is required.\n",
    "                If additional information is necessary, specify what information is needed as a simple, direct question.\n",
    "\n",
    "                Query: {query}\n",
    "                Initial Response: {initial_response}\n",
    "\n",
    "                Is additional information required? If no, state \"No additional information is required.\" If yes, what specific information is needed? Phrase it as a simple question.\n",
    "            \"\"\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "    def formulate_final_query(self, initial_response: str, additional_info: str, query: str) -> str:\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are an AI assistant tasked with formulating queries based on given information. Do not use approximations or assumptions.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"\n",
    "                Given the following:\n",
    "                1. Original query: {query}\n",
    "                2. Initial response: {initial_response}\n",
    "                3. Additional information: {additional_info}\n",
    "\n",
    "                Formulate a new query that will help generate a complete and accurate final answer. Do not use approximations or assumptions.\n",
    "            \"\"\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "    \n",
    "\n",
    "    def generate_final_answer(self, query: str, initial_response: str, additional_info: str, final_query_response: str) -> str:\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are an AI assistant tasked with generating comprehensive answers based on given information. Stick to the information provided and do not introduce unrelated concepts or data.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"\n",
    "                Given the following information:\n",
    "                1. Original query: {query}\n",
    "                2. Initial response: {initial_response}\n",
    "                3. Additional information gathered: {additional_info}\n",
    "                4. Response to final query: {final_query_response}\n",
    "\n",
    "                Please generate a complete and accurate final answer to the original query, incorporating all relevant information. Do not introduce concepts or data not mentioned in the provided information.\n",
    "            \"\"\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "def create_rag_agent(index: VectorStoreIndex, llm: Groq) -> Tuple[BaseQueryEngine, Groq]:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=5,\n",
    "        node_postprocessors=[\n",
    "            FlagEmbeddingReranker(\n",
    "                top_n=3,\n",
    "                model=\"BAAI/bge-reranker-large\",\n",
    "            )\n",
    "        ],\n",
    "        verbose=True\n",
    "    )\n",
    "    return query_engine, llm\n",
    "\n",
    "def process_advanced_query(query_engine: BaseQueryEngine, llm: Groq, query: str) -> str:\n",
    "    processor = QueryProcessor(query_engine)\n",
    "    analyzer = InformationAnalyzer(llm)\n",
    "\n",
    "    # Step 1: Generate initial response\n",
    "    initial_result = processor.process_query(query)\n",
    "    initial_response = initial_result['response']\n",
    "    print(f\"Initial Response: {initial_response}\")\n",
    "\n",
    "    # Step 2: Check if additional information is required\n",
    "    analysis_result = analyzer.analyze_response(initial_response, query)\n",
    "    print(f\"Analysis Result: {analysis_result}\")\n",
    "\n",
    "    if \"no additional information is required\" in analysis_result.lower():\n",
    "        return initial_response\n",
    "\n",
    "    # Step 3: Fetch additional required information\n",
    "    additional_info_result = processor.process_query(analysis_result)\n",
    "    additional_info = additional_info_result['response']\n",
    "    print(f\"Additional Information: {additional_info}\")\n",
    "\n",
    "    # Step 4: Formulate new query and generate final answer\n",
    "    final_query = analyzer.formulate_final_query(initial_response, additional_info, query)\n",
    "    print(f\"Final Query: {final_query}\")\n",
    "    \n",
    "    final_query_result = processor.process_query(final_query)\n",
    "    final_query_response = final_query_result['response']\n",
    "    print(f\"Final Query Response: {final_query_response}\")\n",
    "\n",
    "    # Step 5: Generate comprehensive final answer\n",
    "    final_answer = analyzer.generate_final_answer(query, initial_response, additional_info, final_query_response)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    agent = create_rag_agent(index_markdown_element, llm)\n",
    "    \n",
    "    ques = \"Two children are at opposite ends of an steel rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in steel to reach the second child.\"\n",
    "    result = process_advanced_query(agent, ques)\n",
    "    print(f\"Final Query Result: {result}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred in the main execution: {str(e)}\")\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "2024-09-29 10:54:29,375 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response: The speed of sound in a medium is a characteristic of that medium. Since the speed of sound in aluminium and steel are different, the time taken by the sound wave to reach the second child will also be different.\n",
      "\n",
      "Let's assume the length of each rod is L. Let the speed of sound in aluminium be v_al and in steel be v_st. \n",
      "\n",
      "The time taken by the sound wave in aluminium to reach the second child is t_al = L / v_al. \n",
      "\n",
      "The time taken by the sound wave in steel to reach the second child is t_st = L / v_st.\n",
      "\n",
      "The ratio of times taken by the sound wave in aluminium and in steel to reach the second child is t_al / t_st = (L / v_al) / (L / v_st) = v_st / v_al.\n",
      "\n",
      "This ratio depends on the speeds of sound in aluminium and steel, which are characteristic properties of these materials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 10:54:30,018 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result: No additional information is required.\n",
      "\n",
      "Final Answer: The speed of sound in a medium is a characteristic of that medium. Since the speed of sound in aluminium and steel are different, the time taken by the sound wave to reach the second child will also be different.\n",
      "\n",
      "Let's assume the length of each rod is L. Let the speed of sound in aluminium be v_al and in steel be v_st. \n",
      "\n",
      "The time taken by the sound wave in aluminium to reach the second child is t_al = L / v_al. \n",
      "\n",
      "The time taken by the sound wave in steel to reach the second child is t_st = L / v_st.\n",
      "\n",
      "The ratio of times taken by the sound wave in aluminium and in steel to reach the second child is t_al / t_st = (L / v_al) / (L / v_st) = v_st / v_al.\n",
      "\n",
      "This ratio depends on the speeds of sound in aluminium and steel, which are characteristic properties of these materials.\n"
     ]
    }
   ],
   "source": [
    "query_engine, llm = create_rag_agent(index_markdown_element, llm)\n",
    "\n",
    "my_question = \"Two children are at opposite ends of a steel rod and an aluminium rod. One strikes the end of each rod with a stone. Find the ratio of times taken by the sound wave in aluminium and in steel to reach the second child.\"\n",
    "\n",
    "result = process_advanced_query(query_engine, llm, my_question)\n",
    "print(f\"\\nFinal Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.96it/s]\n",
      "2024-09-29 09:47:10,528 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response: (a) Infrasound: Frequencies below 20 Hz\n",
      "(b) Ultrasound: Frequencies above 20 kHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 09:47:10,885 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result: No additional information is required.\n",
      "\n",
      "Final Answer: (a) Infrasound: Frequencies below 20 Hz\n",
      "(b) Ultrasound: Frequencies above 20 kHz\n"
     ]
    }
   ],
   "source": [
    "another_question = \"\"\"What is the range of frequencies associated with:\n",
    "(a) Infrasound?\n",
    "(b) Ultrasound?\"\"\"\n",
    "result = process_advanced_query(query_engine, llm, another_question)\n",
    "print(f\"\\nFinal Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "2024-09-29 09:51:33,399 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response: Two practical applications of reflection of sound waves are ultrasonography, which is used for medical imaging and examination of the foetus during pregnancy, and breaking small 'stones' formed in the kidneys into fine grains that can be flushed out with urine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 09:51:33,794 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result: No additional information is required.\n",
      "\n",
      "Final Answer: Two practical applications of reflection of sound waves are ultrasonography, which is used for medical imaging and examination of the foetus during pregnancy, and breaking small 'stones' formed in the kidneys into fine grains that can be flushed out with urine.\n"
     ]
    }
   ],
   "source": [
    "new_ques = \"Give two practical applications of reflection of sound waves.\"\n",
    "new_result = process_advanced_query(query_engine, llm, new_ques)\n",
    "print(f\"\\nFinal Answer: {new_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEngine:\n",
    "    def __init__(self, index: VectorStoreIndex):\n",
    "        self.query_engine = index.as_query_engine(\n",
    "            similarity_top_k=5,\n",
    "            node_postprocessors=[\n",
    "                FlagEmbeddingReranker(\n",
    "                    top_n=3,\n",
    "                    model=\"BAAI/bge-reranker-large\",\n",
    "                )\n",
    "            ],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def query(self, query: str) -> Dict[str, str]:\n",
    "        print(f\"QueryEngine: Executing query: {query}\")\n",
    "        response = self.query_engine.query(query)\n",
    "        print(f\"QueryEngine: Response received\")\n",
    "        return {\n",
    "            \"response\": response.response,\n",
    "            \"source_nodes\": [node.node.get_content() for node in response.source_nodes],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedQueryAgent:\n",
    "    def __init__(self, query_engine: QueryEngine, llm: Groq):\n",
    "        self.query_engine = query_engine\n",
    "        self.llm = llm\n",
    "\n",
    "    def analyze_response(self, initial_response: str, query: str) -> str:\n",
    "        print(f\"AdvancedQueryAgent: Analyzing response for query: {query}\")\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"Analyze the response and determine if additional information is needed. If so, specify it as a simple, direct question without using any variables or abbreviations. Use natural language to ask for the required information.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"Query: {query}\\nInitial Response: {initial_response}\\n\\nIs additional information required? If yes, what specific information is needed? Please provide the follow-up question in simple, clear language.\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        print(f\"AdvancedQueryAgent: Analysis complete\")\n",
    "        return response.message.content\n",
    "\n",
    "    def get_additional_info(self, analysis: str) -> Dict[str, str]:\n",
    "        print(f\"AdvancedQueryAgent: Getting additional info based on analysis: {analysis}\")\n",
    "        return self.query_engine.query(analysis)\n",
    "\n",
    "    def generate_final_answer(self, query: str, initial_response: str, additional_info: str) -> str:\n",
    "        print(f\"AdvancedQueryAgent: Generating final answer for query: {query}\")\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"Generate a comprehensive final answer based on the given information.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"Original query: {query}\\nInitial response: {initial_response}\\nAdditional information: {additional_info}\\n\\nPlease provide a complete and accurate final answer.\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        print(f\"AdvancedQueryAgent: Final answer generated\")\n",
    "        return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, index: VectorStoreIndex, llm: Groq):\n",
    "        self.query_engine = QueryEngine(index)\n",
    "        self.advanced_agent = AdvancedQueryAgent(self.query_engine, llm)\n",
    "\n",
    "    def basic_query(self, query: str) -> str:\n",
    "        print(f\"RAGSystem: Executing basic query: {query}\")\n",
    "        result = self.query_engine.query(query)\n",
    "        print(f\"RAGSystem: Basic query complete\")\n",
    "        return result[\"response\"]\n",
    "\n",
    "    def advanced_query(self, query: str, initial_response: str) -> str:\n",
    "        print(f\"RAGSystem: Starting advanced query process for: {query}\")\n",
    "        analysis = self.advanced_agent.analyze_response(initial_response, query)\n",
    "        print(f\"RAGSystem: Analysis result: {analysis}\")\n",
    "        \n",
    "        if \"no additional information is required\" in analysis.lower():\n",
    "            print(\"RAGSystem: No additional information required. Returning initial response.\")\n",
    "            return initial_response\n",
    "\n",
    "        additional_info = self.advanced_agent.get_additional_info(analysis)\n",
    "        print(f\"RAGSystem: Additional info retrieved: {additional_info['response']}\")\n",
    "        \n",
    "        final_answer = self.advanced_agent.generate_final_answer(query, initial_response, additional_info[\"response\"])\n",
    "        print(\"RAGSystem: Advanced query process complete\")\n",
    "        return final_answer\n",
    "\n",
    "def main(index: VectorStoreIndex, llm: Groq):\n",
    "    rag_system = RAGSystem(index, llm)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        print(\"\\n--- Starting Basic Query ---\")\n",
    "        initial_response = rag_system.basic_query(query)\n",
    "        print(f\"\\nInitial response: {initial_response}\\n\")\n",
    "\n",
    "        user_choice = input(\"Is this answer sufficient? (yes/no): \")\n",
    "        if user_choice.lower() == 'no':\n",
    "            print(\"\\n--- Starting Advanced Query ---\")\n",
    "            final_answer = rag_system.advanced_query(query, initial_response)\n",
    "            print(f\"\\nFinal answer: {final_answer}\\n\")\n",
    "        else:\n",
    "            print(\"Great! Let me know if you have any more questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Basic Query ---\n",
      "RAGSystem: Executing basic query: what sound does the cow makes ? \n",
      "QueryEngine: Executing query: what sound does the cow makes ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.94it/s]\n",
      "2024-09-30 02:53:44,158 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryEngine: Response received\n",
      "RAGSystem: Basic query complete\n",
      "\n",
      "Initial response: There is no information about the sound a cow makes in the provided context. The context discusses sound waves, their properties, and how they are produced and propagated, but it does not mention the specific sound made by a cow.\n",
      "\n",
      "Great! Let me know if you have any more questions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Assume index and llm are already initialized outside this script\n",
    "    # You would typically import them or pass them as arguments\n",
    "    main(index_markdown_element, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step, Event\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.llms import MessageRole\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "import serpapi\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi.google_search import GoogleSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_TEXT = \"\"\"Concise summary:\n",
    "This document covers the fundamentals of sound, including:\n",
    "Production and propagation of sound waves\n",
    "Wave characteristics: wavelength, frequency, amplitude, speed\n",
    "Sound reflection, echo, and reverberation\n",
    "Speed of sound in various media\n",
    "Human hearing range (20 Hz - 20 kHz)\n",
    "Infrasound and ultrasound\n",
    "Applications of ultrasound in medicine and industry\n",
    "Key concepts include vibrating objects as sound sources, longitudinal waves, compressions and rarefactions, reflection laws, and factors affecting sound speed and perception. The text provides a comprehensive introduction to acoustics suitable for high school students.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 03:12:48,484 - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "c:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-01 03:13:18,466 - INFO - Use pytorch device_name: cpu\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Load a model for similarity calculation\n",
    "\n",
    "summary_text = \"\"\"\n",
    "The document covers sound wave production, propagation, wavelength, frequency, amplitude, \n",
    "sound speed, reflection, echo, reverberation, speed of sound in various media, human hearing \n",
    "range (20 Hz - 20 kHz), infrasound, ultrasound, and applications in medicine and industry. \n",
    "It also discusses vibrating objects, longitudinal waves, compressions, rarefactions, reflection laws, \n",
    "and factors affecting sound speed and perception. The text introduces acoustics for high school students.\n",
    "\"\"\"\n",
    "\n",
    "summary_embedding = model.encode(summary_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def is_query_related_to_context(query: str) -> bool:\n",
    "    \"\"\"#Determines if the query relates to the acoustic context using semantic similarity.\"\"\"\n",
    "    #query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    #similarity_score = util.pytorch_cos_sim(query_embedding, summary_embedding).item()\n",
    "    #print(f\"Similarity score for query '{query}': {similarity_score}\")\n",
    "\n",
    "    # Set a similarity threshold, e.g., 0.5 to determine relevance\n",
    "    #return similarity_score > 0.5\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_query_related_to_context(query: str) -> bool:\n",
    "    return any(keyword.lower() in query.lower() for keyword in [\n",
    "        \"sound\", \"wave\", \"frequency\", \"amplitude\", \"reflection\", \n",
    "        \"ultrasound\", \"medicine\", \"industry\", \"echo\", \"reverberation\", \n",
    "        \"human hearing\", \"infrasound\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def is_query_related_to_context(query: str) -> bool:\n",
    "    return any(keyword.lower() in query.lower() for keyword in [\n",
    "        \"sound\", \"wave\", \"frequency\", \"amplitude\", \"reflection\", \n",
    "        \"ultrasound\", \"medicine\", \"industry\", \"echo\", \"reverberation\", \n",
    "        \"human hearing\", \"infrasound\"\n",
    "    ])\"\"\"\n",
    "\n",
    "serpapi_params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"api_key\": getpass(\"SerpAPI_key: \")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event definitions\n",
    "class InputEvent(Event):\n",
    "    input: str\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    id: str\n",
    "    name: str\n",
    "    params: Dict[str, Any]\n",
    "\n",
    "class UserDecisionEvent(Event):\n",
    "    decision: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool functions\n",
    "async def query_engine(index: VectorStoreIndex, query: str) -> Dict[str, Any]:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"QueryEngine: Executing query: {query}\")\n",
    "    response = query_engine.query(query)\n",
    "    return {\n",
    "        \"response\": response.response,\n",
    "        \"source_nodes\": [node.node.get_content() for node in response.source_nodes],\n",
    "    }\n",
    "\n",
    "async def analyze_response(llm: Groq, initial_response: str, query: str) -> Dict[str, Any]:\n",
    "    print(f\"AdvancedQueryAgent: Analyzing response for query: {query}\")\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"Analyze the initial_response and determine if additional information is needed based in what the query is asking. If so, specify it as a simple, direct question without using any variables or abbreviations. Use natural language to ask for the required information.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Query: {query}\\nInitial Response: {initial_response}\\n\\nIs additional information required? If yes, what specific information is needed? Please provide the follow-up question in simple, clear language.\")\n",
    "    ]\n",
    "    response = llm.chat(messages)\n",
    "    return {\"analysis\": response.message.content}\n",
    "\n",
    "async def generate_final_answer(llm: Groq, query: str, initial_response: str, additional_info: str) -> Dict[str, Any]:\n",
    "    print(f\"AdvancedQueryAgent: Generating final answer for query: {query}\")\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"Generate a comprehensive final answer based on the given information.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Original query: {query}\\nInitial response: {initial_response}\\nAdditional information: {additional_info}\\n\\nPlease provide a complete and accurate final answer.\")\n",
    "    ]\n",
    "    response = llm.chat(messages)\n",
    "    return {\"final_answer\": response.message.content}\n",
    "\n",
    "\n",
    "async def web_search_tool(query: str) -> Dict[str, Any]:\n",
    "    print(f\"Web Search Tool: Searching for query: {query}\")\n",
    "    \n",
    "    search = GoogleSearch({\n",
    "        **serpapi_params,\n",
    "        \"q\": query,\n",
    "        \"num\": 5\n",
    "    })\n",
    "    \n",
    "    results = search.get_dict().get(\"organic_results\", [])\n",
    "    \n",
    "    if results:\n",
    "        contexts = \"\\n---\\n\".join(\n",
    "            [f\"Title: {x['title']}\\nSnippet: {x['snippet']}\\nLink: {x['link']}\" for x in results]\n",
    "        )\n",
    "    else:\n",
    "        contexts = \"No results found.\"\n",
    "    \n",
    "    return {\n",
    "        \"response\": contexts,\n",
    "        \"source\": \"Web\"\n",
    "    }\n",
    "\n",
    "async def handle_human_message(llm: Groq, message: str) -> Dict[str, Any]:\n",
    "    \"\"\"Handles generic human-like interactions like greetings.\"\"\"\n",
    "    print(f\"LLM Agent: Handling human message: {message}\")\n",
    "    \n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"Respond as a friendly assistant. If the message seems like a greeting or informal question, reply accordingly.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Message: {message}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    return {\"response\": response.message.content}\n",
    "\n",
    "\n",
    "\n",
    "async def classify_query(llm: Groq, query: str) -> str:\n",
    "    \"\"\"Classifies the query into greeting, index_search, or web_search.\"\"\"\n",
    "    \n",
    "    print(f\"LLM Router: Classifying query: {query}\")\n",
    "    \n",
    "    # First, check if the query is a greeting using the LLM\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"\"\" You are an expert in routing user queries to the appropriate category: 'greeting', 'index_search', or 'web_search'.\n",
    "\n",
    "        If the user's message is a greeting or a casual interaction, categorize it as 'greeting'.\n",
    "        Use 'index_search' for queries related to sound and its properties. This includes but is not limited to:\n",
    "        Production and propagation of sound waves\n",
    "        Wave characteristics (frequency, amplitude, etc.)\n",
    "        Sound reflection, echo, and reverberation\n",
    "        Speed of sound in different media\n",
    "        Human hearing and perception of sound\n",
    "        Infrasound, ultrasound, and their applications (e.g., medical, industrial)\n",
    "        You do not need to rely strictly on keywords but should focus on whether the query is broadly related to sound and its scientific aspects.\n",
    "        For all other queries that don’t fit these categories, choose 'web_search'.\n",
    "\n",
    "        Your only response should be one of the following: 'greeting', 'index_search', or 'web_search'.\n",
    "\"\"\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Query: {query}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    classification = response.message.content.strip().lower()\n",
    "    print(classification)\n",
    "    return classification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Query Process ---\n",
      "LLM Router: Classifying query: how does sound propogaes ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 11:54:37,277 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_search\n",
      "QueryEngine: Executing query: how does sound propogaes ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.51it/s]\n",
      "2024-10-02 11:54:38,312 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Response: Sound propagates through a medium, such as air, water, or solid objects, in the form of a wave. When an object vibrates, it sets the particles of the medium around it vibrating. These particles then exert a force on adjacent particles, causing them to displace from their equilibrium position. This process continues, allowing the disturbance to travel through the medium. The particles of the medium do not move forward themselves, but the disturbance is carried forward, allowing the sound wave to propagate.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object NoneType can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[352], line 294\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# Assume index and llm are already initialized outside this script\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# You would typically import them or pass them as arguments\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_markdown_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\myenv\\lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\myenv\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[352], line 273\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(index, llm)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_query_event\u001b[38;5;241m.\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m process_initial_response_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag_system\u001b[38;5;241m.\u001b[39mprocess_initial_response(initial_query_event, context)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(process_initial_response_event, StopEvent):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocess_initial_response_event\u001b[38;5;241m.\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[352], line 193\u001b[0m, in \u001b[0;36mRAGSystem.process_initial_response\u001b[1;34m(self, ev, context)\u001b[0m\n\u001b[0;32m    191\u001b[0m initial_response \u001b[38;5;241m=\u001b[39m tool_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    192\u001b[0m context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_response\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m initial_response\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInitial Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the initial response\u001b[39;00m\n\u001b[0;32m    195\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs this response satisfactory? (yes/no): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UserDecisionEvent(decision\u001b[38;5;241m=\u001b[39mdecision)\n",
      "\u001b[1;31mTypeError\u001b[0m: object NoneType can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "# Main RAG System as a Workflow\n",
    "class RAGSystem(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        index: VectorStoreIndex,\n",
    "        llm: Groq,\n",
    "        timeout: int = 20,\n",
    "    ):\n",
    "        super().__init__(*args)\n",
    "        self._timeout = timeout\n",
    "        self.llm = llm\n",
    "        self.index = index\n",
    "\n",
    "        # Ensure the tools have the correct functions and metadata\n",
    "        self.tools = {\n",
    "            \"query_engine\": FunctionTool(\n",
    "                fn=lambda **params: query_engine(self.index, **{k: v for k, v in params.items() if k != \"index\"}),\n",
    "                #fn=lambda query: query_engine(self.index, query=query),\n",
    "                metadata={\"name\": \"query_engine\", \"description\": \"Fetches results from the vector index based on the query.\"}\n",
    "            ),\n",
    "            \"analyze_response\": FunctionTool(\n",
    "                fn=lambda **params: analyze_response(self.llm, **params),\n",
    "                metadata={\"name\": \"analyze_response\", \"description\": \"Analyzes the response and determines if more information is needed.\"}\n",
    "            ),\n",
    "            \"generate_final_answer\": FunctionTool(\n",
    "                fn=lambda **params: generate_final_answer(self.llm, **params),\n",
    "                metadata={\"name\": \"generate_final_answer\", \"description\": \"Generates the final answer based on the analysis.\"}\n",
    "            ),\n",
    "            \"web_search_tool\": FunctionTool(  # Add this for clarity and ensure it is referenced\n",
    "                fn=lambda **params: web_search_tool(params[\"query\"]),\n",
    "                metadata={\"name\": \"web_search_tool\", \"description\": \"Searches the web for the query.\"}\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @step\n",
    "    async def handle_initial_query(self, ev: InputEvent, context: Dict[str, Any]) -> StopEvent | ToolCallEvent:\n",
    "        query = ev.input\n",
    "        context[\"original_query\"] = query  # Store the query in context\n",
    "\n",
    "\n",
    "        classification = await classify_query(self.llm, query)\n",
    "\n",
    "        if classification == \"greeting\":\n",
    "            print(\"\\n--- Detected a human message. Invoking LLM agent ---\")\n",
    "            response = await handle_human_message(self.llm, query)\n",
    "            print(f\"LLM response: {response['response']}\")\n",
    "            return StopEvent(result={\"message\": response['response']})\n",
    "\n",
    "        elif classification == \"index_search\":\n",
    "            return ToolCallEvent(\n",
    "                id=\"initial_query\",\n",
    "                name=\"query_engine\",\n",
    "                params={\"index\": self.index, \"query\": query},\n",
    "            )\n",
    "        else :\n",
    "            print(\"\\n--- Query is not related to the context. Initiating web search ---\")\n",
    "            return ToolCallEvent(\n",
    "                id=\"web_search\",\n",
    "                name=\"web_search_tool\",\n",
    "                params={\"query\": query}\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    async def process_initial_response(self, ev: ToolCallEvent, context: Dict[str, Any]) -> StopEvent | UserDecisionEvent:\n",
    "        \n",
    "        if isinstance(ev, StopEvent):\n",
    "            print(\"Received StopEvent, no further processing needed.\")\n",
    "            return ev\n",
    "        if ev.id ==\"web_search\":\n",
    "            tool_output = await self.tools[ev.name].fn(**ev.params)  # Call the appropriate tool function\n",
    "            initial_response = tool_output[\"response\"]\n",
    "            context[\"initial_response\"] = initial_response\n",
    "            print(f\"\\nInitial Response: {initial_response}\")  # Print the initial response\n",
    "            return StopEvent(result={\"message\": \"Web search results provided.\"})\n",
    "\n",
    "        if ev.id == \"initial_query\":\n",
    "            tool_output = await self.tools[ev.name].fn(**ev.params)  # Call the appropriate tool function\n",
    "            initial_response = tool_output[\"response\"]\n",
    "            context[\"initial_response\"] = initial_response\n",
    "            print(f\"\\nInitial Response: {initial_response}\")  # Print the initial response\n",
    "\n",
    "            decision = input(\"Is this response satisfactory? (yes/no): \").strip().lower()\n",
    "            return UserDecisionEvent(decision=decision)\n",
    "\n",
    "    @step\n",
    "    async def handle_user_decision(self, ev: UserDecisionEvent, context: Dict[str, Any]) -> StopEvent | ToolCallEvent:\n",
    "        try:\n",
    "            \n",
    "            if ev.decision == 'yes':\n",
    "                print(\"\\nThank you for confirming. Exiting the program.\")\n",
    "                return StopEvent(result={\"message\": \"User confirmed satisfactory response.\"})  # Exit with message\n",
    "            \n",
    "            else:\n",
    "                print(\"\\n--- Starting Advanced Query ---\")\n",
    "                return ToolCallEvent(\n",
    "                    id=\"analyze_response\",\n",
    "                    name=\"analyze_response\",\n",
    "                    params={\n",
    "                        \"initial_response\": context[\"initial_response\"], \n",
    "                        \"query\": context[\"original_query\"]\n",
    "                    },\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in handling user decision: {e}\")\n",
    "            return StopEvent(result={\"message\": \"An error occurred in user decision handling.\"})\n",
    "\n",
    "    @step\n",
    "    async def process_analysis(self, ev: ToolCallEvent, context: Dict[str, Any]) -> ToolCallEvent:\n",
    "        analysis_output = await self.tools[ev.name].fn(**ev.params)\n",
    "        analysis = analysis_output[\"analysis\"]\n",
    "        print(f\"RAGSystem: Analysis result: {analysis}\")\n",
    "        context[\"follow_up_query\"] = analysis  # Store the follow-up query in context\n",
    "        \n",
    "        return ToolCallEvent(\n",
    "            id=\"additional_query\",\n",
    "            name=\"query_engine\",\n",
    "            params={\"query\": analysis},  # Pass follow-up query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def fetch_additional_info(self, ev: ToolCallEvent, context: Dict[str, Any]) -> ToolCallEvent:\n",
    "        additional_info = await self.tools[ev.name].fn(**ev.params)\n",
    "        print(f\"RAGSystem: Additional info retrieved: {additional_info['response']}\")\n",
    "        context[\"additional_info\"] = additional_info[\"response\"]  # Store the additional info in context\n",
    "        \n",
    "        return ToolCallEvent(\n",
    "            id=\"generate_final_answer\",\n",
    "            name=\"generate_final_answer\",\n",
    "            params={\n",
    "                \"query\": context[\"original_query\"],  # Fetch original query from context\n",
    "                \"initial_response\": context[\"initial_response\"],  # Fetch initial response from context\n",
    "                \"additional_info\": additional_info[\"response\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def generate_final_answer(self, ev: ToolCallEvent, context: Dict[str, Any]) -> StopEvent:\n",
    "        final_answer = await self.tools[ev.name].fn(**ev.params)\n",
    "        return StopEvent(result={\"response\": final_answer[\"final_answer\"]})\n",
    "\n",
    "\n",
    "async def main(index: VectorStoreIndex, llm: Groq):\n",
    "    rag_system = RAGSystem(index=index, llm=llm)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        # Initialize context\n",
    "        context = {}\n",
    "\n",
    "        print(\"\\n--- Starting Query Process ---\")\n",
    "        \n",
    "        initial_query_event = await rag_system.handle_initial_query(InputEvent(input=query), context)\n",
    "        if isinstance(initial_query_event,StopEvent):\n",
    "            print(f\"\\nFinal message: {initial_query_event.result['message']}\\n\")\n",
    "            continue\n",
    "\n",
    "        process_initial_response_event = await rag_system.process_initial_response(initial_query_event, context)\n",
    "        if isinstance(process_initial_response_event, StopEvent):\n",
    "            print(f\"\\nFinal message: {process_initial_response_event.result['message']}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Pass context through the workflow\n",
    "        user_decision_event = await rag_system.handle_user_decision(process_initial_response_event, context)\n",
    "        if isinstance(user_decision_event, StopEvent):\n",
    "            print(f\"\\nFinal message: {user_decision_event.result['message']}\\n\")\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            analysis_event = await rag_system.process_analysis(user_decision_event, context)\n",
    "            additional_info_event = await rag_system.fetch_additional_info(analysis_event, context)\n",
    "            final_answer_event = await rag_system.generate_final_answer(additional_info_event, context)\n",
    "            print(f\"\\nFinal answer: {final_answer_event.result['response']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume index and llm are already initialized outside this script\n",
    "    # You would typically import them or pass them as arguments\n",
    "    import asyncio\n",
    "    asyncio.run(main(index_markdown_element, llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"You are an expert in routing user queries to the appropriate category: 'greeting', 'index_search', or 'web_search'.\n",
    "\n",
    "If the user's message is a greeting or a casual interaction, categorize it as 'greeting'.\n",
    "Use 'index_search' for queries related to sound and its properties. This includes but is not limited to:\n",
    "Production and propagation of sound waves\n",
    "Wave characteristics (frequency, amplitude, etc.)\n",
    "Sound reflection, echo, and reverberation\n",
    "Speed of sound in different media\n",
    "Human hearing and perception of sound\n",
    "Infrasound, ultrasound, and their applications (e.g., medical, industrial)\n",
    "You do not need to rely strictly on keywords but should focus on whether the query is broadly related to sound and its scientific aspects.\n",
    "For all other queries that don’t fit these categories, choose 'web_search'.\n",
    "\n",
    "Your only response should be one of the following: 'greeting', 'index_search', or 'web_search'.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from typing import List\n",
    "from pydantic import Field\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from typing import Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional\n",
    "from llama_index.llms.groq import Groq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#from llama_index.llm_predictor.base import BaseLLM\n",
    "\n",
    "#from llama_index.output_parsers.llm import LLMResult, Generation\n",
    "#from llama_index.callbacks.base import CallbackManagerForLLMRun\n",
    "\n",
    "# Wrapper class to adapt LlamaIndex VectorStoreIndex to LangChain BaseRetriever interface\n",
    "class LlamaIndexVectorStoreRetriever(BaseRetriever):\n",
    "    vector_store_index: VectorStoreIndex = Field(description=\"The LlamaIndex VectorStoreIndex\")\n",
    "    similarity_top_k: int = Field(default=5, description=\"Number of top similar documents to retrieve\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        retriever = self.vector_store_index.as_retriever(similarity_top_k=self.similarity_top_k)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        return [Document(page_content=node.node.get_content(), metadata=node.node.metadata) for node in nodes]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Implement async version if needed\n",
    "        return self._get_relevant_documents(query)\"\"\"\n",
    "\n",
    "\"\"\"class State:\n",
    "    #The state of the workflow.\n",
    "    def __init__(\n",
    "        self,\n",
    "        query: str,\n",
    "        initial_response: str,\n",
    "        additional_info: str = \"\",\n",
    "        final_answer: str = \"\",\n",
    "        needs_info: bool = False\n",
    "    ):\n",
    "        self.query = query\n",
    "        self.initial_response = initial_response\n",
    "        self.additional_info = additional_info\n",
    "        self.final_answer = final_answer\n",
    "        self.needs_info = needs_info\n",
    "\n",
    "class WorkflowState(BaseModel):\n",
    "    query: str\n",
    "    initial_response: str\n",
    "    additional_info: str = \"\"\n",
    "    final_answer: str = \"\"\n",
    "    needs_info: bool = False\n",
    "\n",
    "\n",
    "class GroqLLMWrapper(BaseLLM, BaseModel):\n",
    "    groq_llm: Groq = Field(..., description=\"The Groq LLM instance\")\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            response = self.groq_llm.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",  # Already passed in Groq instance\n",
    "                stop=stop,\n",
    "                **kwargs\n",
    "            )\n",
    "            generations.append([Generation(text=response.choices[0].message.content)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"groq\"\n",
    "\n",
    "    def predict_messages(self, messages: List[str], **kwargs) -> str:\n",
    "    # Convert HumanMessage objects to their text content\n",
    "        prompt = \" \".join([msg.content if isinstance(msg, HumanMessage) else msg for msg in messages])\n",
    "        response = self._generate([prompt], **kwargs)\n",
    "        return response.generations[0][0].text\"\"\"\n",
    "\n",
    "\n",
    "#def check_completeness(state: WorkflowState, llm: GroqLLMWrapper) -> Dict[str, WorkflowState]:\n",
    "#    prompt = ChatPromptTemplate.from_template(\n",
    "#        \"\"\"Given the following query and initial response, determine if the response contains \n",
    "\"\"\"        all necessary information to fully answer the query. If not, specify what additional \n",
    "        information is needed.\n",
    "\n",
    "        Query: {query}\n",
    "        Initial Response: {initial_response}\n",
    "\n",
    "        Output your response in the following format:\n",
    "        Complete: [Yes/No]\n",
    "        Additional Info Needed: [If No, specify what additional information is needed]\n",
    "        \"\"\"\n",
    "#    )\n",
    "    \n",
    "#    parser = StructuredOutputParser.from_response_schemas([\n",
    "#        ResponseSchema(name=\"Complete\", description=\"Whether the response is complete (Yes/No)\"),\n",
    "#        ResponseSchema(name=\"Additional Info Needed\", description=\"What additional information is needed, if any\")\n",
    "#    ])\n",
    "    \n",
    "#    response = llm.predict_messages(prompt.format_messages(\n",
    "#        query=state.query, \n",
    "#        initial_response=state.initial_response\n",
    "#    ))\n",
    "    \n",
    "#    parsed_response = parser.parse(response)\n",
    "#    state.needs_info = parsed_response[\"Complete\"].lower() == \"no\"\n",
    "#    if state.needs_info:\n",
    "#        state.additional_info = parsed_response[\"Additional Info Needed\"]\n",
    "    \n",
    "#    return {\"state\": state}\n",
    "\n",
    "\"\"\"def retrieve_additional_info(state: WorkflowState, retriever: LlamaIndexVectorStoreRetriever) -> Dict[str, WorkflowState]:\n",
    "    #Retrieve additional information if needed.\n",
    "    if state.needs_info:\n",
    "        additional_info_query = f\"Provide information about: {state.additional_info}\"\n",
    "        docs = retriever.get_relevant_documents(additional_info_query)\n",
    "        state.additional_info = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"state\": state}\"\"\"\n",
    "\n",
    "#def generate_final_answer(state: WorkflowState, llm: GroqLLMWrapper) -> Dict[str, WorkflowState]:\n",
    "#    prompt = ChatPromptTemplate.from_template(\n",
    "#        \"\"\"Given the following query, initial response, and additional information (if any),\n",
    "#        generate a complete and accurate answer.0\n",
    "\n",
    "#        Query: {query}\n",
    "#        Initial Response: {initial_response}\n",
    " #       Additional Information: {additional_info}\n",
    "\n",
    "#        Complete Answer:\n",
    "#        \"\"\"\n",
    "#    )\n",
    "    \n",
    "#    state.final_answer = llm.predict_messages(prompt.format_messages(\n",
    "#        query=state.query,\n",
    "#        initial_response=state.initial_response,\n",
    "#        additional_info=state.additional_info\n",
    "#    ))\n",
    "    \n",
    "#    return {\"state\": state}\n",
    "\n",
    "\"\"\"def create_workflow(llm: BaseLLM, retriever: LlamaIndexVectorStoreRetriever) -> StateGraph:\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"check_completeness\", lambda state: check_completeness(state, llm))\n",
    "    workflow.add_node(\"retrieve_additional_info\", lambda state: retrieve_additional_info(state, retriever))\n",
    "    workflow.add_node(\"generate_final_answer\", lambda state: generate_final_answer(state, llm))\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.add_edge(\"check_completeness\", \"retrieve_additional_info\")\n",
    "    workflow.add_edge(\"retrieve_additional_info\", \"generate_final_answer\")\n",
    "    workflow.add_edge(\"generate_final_answer\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"check_completeness\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "def solve_query_with_workflow(query: str, initial_response: str, groq_llm, vector_store_index: VectorStoreIndex) -> str:\n",
    "    # Pass groq_llm as a keyword argument\n",
    "    llm = GroqLLMWrapper(groq_llm=groq_llm)  \n",
    "    retriever_wrapper = LlamaIndexVectorStoreRetriever(vector_store_index=vector_store_index, similarity_top_k=5)\n",
    "    workflow = create_workflow(llm, retriever_wrapper)\n",
    "    \n",
    "    # Create the runnable graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Prepare the initial state\n",
    "    initial_state = WorkflowState(\n",
    "        query=query,\n",
    "        initial_response=initial_response\n",
    "    )\n",
    "    \n",
    "    # Run the workflow\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    return final_state.final_answer\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def index_setup(nodes, embed_model, db_path=\"./chroma_db\"):\n",
    "    # Create or connect to the persistent ChromaDB\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "    # Create ChromaVectorStore\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # Create storage context and index\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, storage_context=storage_context, embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    return index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def load_index(embed_model, db_path=\"./chroma_db\"):\n",
    "    # Load the index from persistent storage\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    # Create the index from the vector store\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    return index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes_index = index_setup(nodes,embed_model, db_path=\"./chroma_db_nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"query_engine = nodes_index.as_query_engine(llm=llm\n",
    "    ,similarity_top_k=3, verbose=True\n",
    ")\n",
    "\n",
    "question1 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "\n",
    "response = query_engine.query(question1)\n",
    "print(response.source_nodes)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk , nodes \n",
    "\"\"\"db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_query_engine = index.as_query_engine(llm=llm\n",
    "    ,similarity_top_k=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is sound wave called a longitudinal wave?\"\n",
    "response = recursive_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[-2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[-2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "1 : \"What is sound and how is it produced?\",\n",
    "\n",
    "2: \"Describe with the help of a diagram, how compressions and rarefactions are produced in air near a source of sound.\",\n",
    "\n",
    "3: \"Why is sound wave called a longitudinal wave?\",\n",
    "\n",
    "4: \"Which characteristic of the sound helps you to identify your friend by his voice while sitting with others in a dark room?\",\n",
    "\n",
    "5: \"Flash and thunder are produced simultaneously. But thunder is heard a few seconds after the flash is seen, why?\",\n",
    "\n",
    "6: \"A person has a hearing range from 20 Hz to 20 kHz. What are the typical wavelengths of sound waves in air corresponding to these two frequencies? Take the speed of sound in air as 344 m s⁻¹.\",\n",
    "\n",
    "7: \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\",\n",
    "\n",
    "8: \"The frequency of a source of sound is 100 Hz. How many times does it vibrate in a minute?\",\n",
    "\n",
    "9: \"Does sound follow the same laws of reflection as light does? Explain.\",\n",
    "\n",
    "10: \"When a sound is reflected from a distant object, an echo is produced. Let the distance between the reflecting surface and the source of sound production remains the same. Do you hear echo sound on a hotter day?\",\n",
    "\n",
    "11: \"Give two practical applications of reflection of sound waves.\",\n",
    "\n",
    "12: \"A stone is dropped from the top of a tower 500 m high into a pond of water at the base of the tower. When is the splash heard at the top? Given, g = 10 m s⁻² and speed of sound = 340 m s⁻¹.\",\n",
    "\n",
    "13: \"A sound wave travels at a speed of 339 m s⁻¹. If its wavelength is 1.5 cm, what is the frequency of the wave? Will it be audible?\",\n",
    "\n",
    "14: \"What is reverberation? How can it be reduced?\",\n",
    "\n",
    "15: \"What is loudness of sound? What factors does it depend on?\",\n",
    "\n",
    "16: \"How is ultrasound used for cleaning?\",\n",
    "\n",
    "17: \"Explain how defects in a metal block can be detected using ultrasound.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for value in questions.values(): \n",
    "    repo = recursive_query_engine.query(value)\n",
    "    answers.append(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "for key, value in questions.items():\n",
    "    ans[key] = recursive_query_engine.query(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
