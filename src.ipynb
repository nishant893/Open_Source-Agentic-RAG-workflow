{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"LLamaParse API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ_API_KEY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    num_workers=1,\n",
    "    premium_mode = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it]\n"
     ]
    }
   ],
   "source": [
    "documents = parser.load_data([r\"C:\\Users\\nisha\\Desktop\\RAG_Assign\\RAG_Assign\\iesc111.pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[3].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "nodes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "splitter1 = MarkdownNodeParser(\n",
    "    include_metadata= False,\n",
    "    include_prev_next_rel= True,\n",
    ")\n",
    "\n",
    "nodes_markdown = splitter1.get_nodes_from_documents(documents)\n",
    "\n",
    "nodes_markdown[1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes_markdown_element = node_parser.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexNode(id_='db060258-4878-4705-be5c-386b4978e71f', embedding=None, metadata={'col_schema': 'Column: State\\nType: string\\nSummary: None\\n\\nColumn: Substance\\nType: string\\nSummary: None\\n\\nColumn: Speed in m/s\\nType: number\\nSummary: None'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7a502ce8-cada-4f01-b621-6b8f4ae55c54', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='9d0d77c9411a751597c9f7f9be255d1ae84b92311c71dcf545bdc170e8689b54'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='56f63ebd-6025-4439-8286-993512753375', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d8f6433e378187647f66887e09562443b2f6efd17af31115e71054d502320695'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8ea8291c-cc0c-4de4-a8a9-be49d3a6b694', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' State   ': {0: ' Solids  ', 1: '         ', 2: '         ', 3: '         ', 4: '         ', 5: '         ', 6: ' Liquids ', 7: '         ', 8: '         ', 9: '         ', 10: ' Gases   ', 11: '         ', 12: '         ', 13: '         ', 14: '         '}, ' Substance       ': {0: ' Aluminium       ', 1: ' Nickel          ', 2: ' Steel           ', 3: ' Iron            ', 4: ' Brass           ', 5: ' Glass (Flint)   ', 6: ' Water (Sea)     ', 7: ' Water (distilled)', 8: ' Ethanol         ', 9: ' Methanol        ', 10: ' Hydrogen        ', 11: ' Helium          ', 12: ' Air             ', 13: ' Oxygen          ', 14: ' Sulphur dioxide '}, ' Speed in m/s ': {0: 6420, 1: 6040, 2: 5960, 3: 5950, 4: 4700, 5: 3980, 6: 1531, 7: 1498, 8: 1207, 9: 1103, 10: 1284, 11: 965, 12: 346, 13: 316, 14: 213}}\", 'table_summary': 'Speed of sound in different substances,\\nwith the following table title:\\nSpeed of sound in different substances,\\nwith the following columns:\\n- State: None\\n- Substance: None\\n- Speed in m/s: None\\n'}, hash='4fbbe60bcc87e8f64bf44087080e1099764bc1a1c75d9aeaab037b41c456f2cc')}, text='Speed of sound in different substances,\\nwith the following table title:\\nSpeed of sound in different substances,\\nwith the following columns:\\n- State: None\\n- Substance: None\\n- Speed in m/s: None\\n', mimetype='text/plain', start_char_idx=969, end_char_idx=1734, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='8ea8291c-cc0c-4de4-a8a9-be49d3a6b694', obj=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_markdown_element[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes_markdown_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_markdown_element = VectorStoreIndex(nodes=base_nodes+objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-postprocessor-flag-embedding-reranker git+https://github.com/FlagOpen/FlagEmbedding.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=3,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_element_query_engine = index_markdown_element.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 8ea8291c-cc0c-4de4-a8a9-be49d3a6b694: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the speed of sound in aluminium?\n",
      "\u001b[0mThe speed of sound in aluminium is 6420 m/s.\n"
     ]
    }
   ],
   "source": [
    "eq = \"What is the speed of sound in aluminium?\"\n",
    "\n",
    "response_element1 = markdown_element_query_engine.query(eq)\n",
    "print(response_element1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq2 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "response_element2 = markdown_element_query_engine.query(eq2)\n",
    "print(response_element2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowState:\n",
    "    def __init__(self, query: str, initial_response: str):\n",
    "        self.query = query\n",
    "        self.initial_response = initial_response\n",
    "        self.additional_info = \"\"\n",
    "        self.final_answer = \"\"\n",
    "        self.needs_info = False\n",
    "\n",
    "\n",
    "def check_completeness(state: WorkflowState, llm: Groq) -> WorkflowState:\n",
    "    prompt = f\"\"\"\n",
    "    Given the following query and initial response, determine if the response contains \n",
    "    all necessary information to fully answer the query. If not, specify what additional \n",
    "    information is needed.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "\n",
    "    Output your response in the following format:\n",
    "    Complete: [Yes/No]\n",
    "    Additional Info Needed: [If No, specify what additional information is needed]\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    # Extract 'Complete' and 'Additional Info Needed' fields from response\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'Complete' in line:\n",
    "            state.needs_info = 'No' in line\n",
    "        elif 'Additional Info Needed' in line and state.needs_info:\n",
    "            state.additional_info = line.split(': ')[1].strip()\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def retrieve_additional_info(state: WorkflowState, query_engine: RetrieverQueryEngine) -> WorkflowState:\n",
    "    if state.needs_info:\n",
    "        additional_info_query = state.additional_info\n",
    "        response = query_engine.query(additional_info_query)\n",
    "\n",
    "        if isinstance(response, str):\n",
    "            state.additional_info = response.strip()\n",
    "        else:\n",
    "            raise TypeError(\"Expected string response from query engine.\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_final_answer(state: WorkflowState, llm: Groq) -> WorkflowState:\n",
    "    prompt = f\"\"\"\n",
    "    Given the following query, initial response, and additional information (if any),\n",
    "    generate a complete and accurate answer.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "    Additional Information: {state.additional_info}\n",
    "\n",
    "    Complete Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        state.final_answer = response.strip()\n",
    "    else:\n",
    "        raise TypeError(\"Expected string response from generate_final_answer.\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def create_workflow(query_engine: RetrieverQueryEngine, llm: Groq):\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: check_completeness(x, llm),\n",
    "            name=\"check_completeness\",\n",
    "            description=\"Check if the initial response is complete\"\n",
    "        ),\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: retrieve_additional_info(x, query_engine),\n",
    "            name=\"retrieve_additional_info\",\n",
    "            description=\"Retrieve additional information if needed\"\n",
    "        ),\n",
    "        FunctionTool.from_defaults(\n",
    "            fn=lambda x: generate_final_answer(x, llm),\n",
    "            name=\"generate_final_answer\",\n",
    "            description=\"Generate the final answer\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=llm,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def solve_query_with_workflow(query: str, query_engine: RetrieverQueryEngine, llm: Groq) -> str:\n",
    "    # Get initial response\n",
    "    initial_response = query_engine.query(query)\n",
    "\n",
    "    # Create initial state\n",
    "    state = WorkflowState(query=query, initial_response=str(initial_response))\n",
    "\n",
    "    # Create and run the workflow\n",
    "    agent = create_workflow(query_engine, llm)\n",
    "\n",
    "    # Define the task for the agent\n",
    "    task = f\"\"\"\n",
    "    Given the following query and initial response, use the available tools to ensure a complete and accurate answer is provided.\n",
    "    If the initial response is incomplete, retrieve additional information and generate a final answer.\n",
    "\n",
    "    Query: {state.query}\n",
    "    Initial Response: {state.initial_response}\n",
    "\n",
    "    Use the tools in this order:\n",
    "    1. check_completeness\n",
    "    2. retrieve_additional_info (if needed)\n",
    "    3. generate_final_answer\n",
    "\n",
    "    Return the final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the agent\n",
    "    response = agent.chat(task)\n",
    "\n",
    "    # Process the response to extract the final answer\n",
    "    if hasattr(response, 'response'):\n",
    "        final_answer = response.response.strip()\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected response format from agent.\")\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.16s/it]\n",
      "2024-09-29 06:21:56,664 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Given the following query and initial response, use the available tools to ensure a complete and accurate answer is provided.\n",
      "    If the initial response is incomplete, retrieve additional information and generate a final answer.\n",
      "\n",
      "    Query: Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\n",
      "    Initial Response: The speed of sound in air is given as 344 m s⁻¹. Let's assume the length of the aluminium rod is 'L'. The time taken by the sound wave to reach the second child in air is t₁ = L / 344. \n",
      "\n",
      "The speed of sound in a medium depends on the nature of the medium. Since the speed of sound in aluminium is not given, let's assume it to be 'v'. The time taken by the sound wave to reach the second child in aluminium is t₂ = L / v. \n",
      "\n",
      "Now, we need to find the ratio of t₁ and t₂. \n",
      "\n",
      "t₁ / t₂ = (L / 344) / (L / v) \n",
      "t₁ / t₂ = v / 344 \n",
      "\n",
      "So, the ratio of times taken by the sound wave in air and in aluminium to reach the second child is v / 344.\n",
      "\n",
      "    Use the tools in this order:\n",
      "    1. check_completeness\n",
      "    2. retrieve_additional_info (if needed)\n",
      "    3. generate_final_answer\n",
      "\n",
      "    Return the final answer.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:21:58,042 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: check_completeness with args: {\"x\":\"The speed of sound in air is given as 344 m s⁻¹. Let's assume the length of the aluminium rod is 'L'. The time taken by the sound wave to reach the second child in air is t₁ = L / 344. The speed of sound in a medium depends on the nature of the medium. Since the speed of sound in aluminium is not given, let's assume it to be 'v'. The time taken by the sound wave to reach the second child in aluminium is t₂ = L / v. Now, we need to find the ratio of t₁ and t₂. t₁ / t₂ = (L / 344) / (L / v) t₁ / t₂ = v / 344. So, the ratio of times taken by the sound wave in air and in aluminium to reach the second child is v / 344.\"}\n",
      "Got output: Error: 'str' object has no attribute 'query'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:21:59,220 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: retrieve_additional_info with args: {\"x\":\"The speed of sound in air is given as 344 m s⁻¹. Let's assume the length of the aluminium rod is 'L'. The time taken by the sound wave to reach the second child in air is t₁ = L / 344. The speed of sound in a medium depends on the nature of the medium. Since the speed of sound in aluminium is not given, let's assume it to be 'v'. The time taken by the sound wave to reach the second child in aluminium is t₂ = L / v. Now, we need to find the ratio of t₁ and t₂. t₁ / t₂ = (L / 344) / (L / v) t₁ / t₂ = v / 344. So, the ratio of times taken by the sound wave in air and in aluminium to reach the second child is v / 344.\"}\n",
      "Got output: Error: 'str' object has no attribute 'needs_info'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:21:59,880 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: retrieve_additional_info with args: {\"x\":\"What is the speed of sound in aluminium?\"}\n",
      "Got output: Error: 'str' object has no attribute 'needs_info'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:22:00,331 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-29 06:22:00,366 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-09-29 06:22:18,999 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: retrieve_additional_info with args: {\"x\":\"The speed of sound in aluminium is approximately 6420 m/s.\"}\n",
      "Got output: Error: 'str' object has no attribute 'needs_info'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:22:19,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-29 06:22:19,334 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-09-29 06:22:40,591 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: generate_final_answer with args: {\"x\":\"The speed of sound in air is given as 344 m s⁻¹. Let's assume the length of the aluminium rod is 'L'. The time taken by the sound wave to reach the second child in air is t₁ = L / 344. The speed of sound in a medium depends on the nature of the medium. Since the speed of sound in aluminium is not given, let's assume it to be 'v'. The time taken by the sound wave to reach the second child in aluminium is t₂ = L / v. Now, we need to find the ratio of t₁ and t₂. t₁ / t₂ = (L / 344) / (L / v) t₁ / t₂ = v / 344. So, the ratio of times taken by the sound wave in air and in aluminium to reach the second child is v / 344. The speed of sound in aluminium is approximately 6420 m/s.\"}\n",
      "Got output: Error: 'str' object has no attribute 'query'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:22:40,953 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-29 06:22:40,953 - INFO - Retrying request to /chat/completions in 22.000000 seconds\n",
      "2024-09-29 06:23:03,614 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: generate_final_answer with args: {\"x\":\"The ratio of times taken by the sound wave in air and in aluminium to reach the second child is 6420 / 344.\"}\n",
      "Got output: Error: 'str' object has no attribute 'query'\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 06:23:03,996 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-29 06:23:03,997 - INFO - Retrying request to /chat/completions in 24.000000 seconds\n",
      "2024-09-29 06:23:28,784 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final answer is: The ratio of times taken by the sound wave in air and in aluminium to reach the second child is 18.6.\n"
     ]
    }
   ],
   "source": [
    "query = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "#logger.info(f\"Processing query: {query}\")\n",
    "final_answer = solve_query_with_workflow(query, markdown_element_query_engine, llm)\n",
    "#logger.info(f\"Final answer: {final_answer}\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the complete and accurate answer:\n",
      "\n",
      "Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\n",
      "\n",
      "Let's assume the length of the aluminium rod is L. The speed of sound in air is 344 m/s. The speed of sound in aluminium is approximately 6420 m/s.\n",
      "\n",
      "Time taken by the sound wave in air = L / 344 m/s\n",
      "Time taken by the sound wave in aluminium = L / 6420 m/s\n",
      "\n",
      "Now, we can find the ratio of the times taken:\n",
      "\n",
      "Ratio = (Time taken by the sound wave in air) / (Time taken by the sound wave in aluminium)\n",
      "= (L / 344 m/s) / (L / 6420 m/s)\n",
      "= (L / 344) × (6420 / L)\n",
      "= 6420 / 344\n",
      "= 18.65\n",
      "\n",
      "So, the ratio of the times taken by the sound wave in air and in aluminium to reach the second child is approximately 18.65. This means that the sound wave in aluminium reaches the second child about 18.65 times faster than the sound wave in air.\n"
     ]
    }
   ],
   "source": [
    "eq2 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "final_answer = solve_query_with_workflow(eq2, markdown_element_query_engine, llm)\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.query_engine import BaseQueryEngine\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, query_engine: BaseQueryEngine):\n",
    "        self.query_engine = query_engine\n",
    "\n",
    "    def process_query(self, query: str) -> Dict[str, str]:\n",
    "        response = self.query_engine.query(query)\n",
    "        return {\n",
    "            \"response\": response.response,\n",
    "            \"source_nodes\": [node.node.get_content() for node in response.source_nodes],\n",
    "        }\n",
    "\n",
    "class InformationAnalyzer:\n",
    "    def __init__(self, groq_llm):\n",
    "        self.llm = groq_llm\n",
    "\n",
    "    def analyze_response(self, initial_response: str, query: str) -> str:\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are an AI assistant tasked with analyzing responses and determining if additional information is needed.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"\n",
    "                Given the following query and initial response, determine if any additional information is required to fully answer the query. If so, specify what additional information is needed.\n",
    "\n",
    "                Query: {query}\n",
    "                Initial Response: {initial_response}\n",
    "\n",
    "                Is additional information required? If yes, what specific information is needed?\n",
    "            \"\"\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "    def formulate_final_query(self, initial_response: str, additional_info: str, query: str) -> str:\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are an AI assistant tasked with formulating queries based on given information.\"),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"\n",
    "                Given the following:\n",
    "                1. Original query: {query}\n",
    "                2. Initial response: {initial_response}\n",
    "                3. Additional information: {additional_info}\n",
    "\n",
    "                Formulate a new query that will help generate a complete and accurate final answer.\n",
    "            \"\"\")\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        return response.message.content\n",
    "\n",
    "def create_rag_agent(index: VectorStoreIndex, llm: Groq) -> tuple:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=5,\n",
    "        node_postprocessors=[\n",
    "            FlagEmbeddingReranker(\n",
    "                top_n=3,\n",
    "                model=\"BAAI/bge-reranker-large\",\n",
    "            )\n",
    "        ],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return query_engine, llm\n",
    "\n",
    "def process_advanced_query(query_engine: BaseQueryEngine, llm: Groq, query: str) -> str:\n",
    "    processor = QueryProcessor(query_engine)\n",
    "    analyzer = InformationAnalyzer(llm)\n",
    "\n",
    "    # Step 1: Generate initial response\n",
    "    initial_result = processor.process_query(query)\n",
    "    initial_response = initial_result['response']\n",
    "\n",
    "    # Step 2: Check if additional information is required\n",
    "    analysis_result = analyzer.analyze_response(initial_response, query)\n",
    "\n",
    "    if \"No additional information is required\" in analysis_result:\n",
    "        return initial_response\n",
    "\n",
    "    # Step 3: Fetch additional required information\n",
    "    additional_info_query = f\"Find information about: {analysis_result}\"\n",
    "    additional_info_result = processor.process_query(additional_info_query)\n",
    "    additional_info = additional_info_result['response']\n",
    "\n",
    "    # Step 4: Formulate new query and generate final answer\n",
    "    final_query = analyzer.formulate_final_query(initial_response, additional_info, query)\n",
    "    final_result = processor.process_query(final_query)\n",
    "\n",
    "    return final_result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "2024-09-29 07:03:38,586 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-29 07:03:39,166 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 8ea8291c-cc0c-4de4-a8a9-be49d3a6b694: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Find information about: Yes, additional information is required to fully answer the query.\n",
      "\n",
      "The specific information needed is the speed of sound in aluminium (v_al). Without knowing the exact value of v_al, the ratio of times taken by the sound wave in air and in aluminium to reach the second child cannot be determined.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 07:04:05,951 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-29 07:04:06,615 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "2024-09-29 07:04:35,871 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Result: Let's break down the problem step by step. We need to find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\n",
      "\n",
      "Let's assume the distance between the two children is 'd'. \n",
      "\n",
      "The time taken by the sound wave in air to reach the second child is t_air = d / v_air, where v_air is the speed of sound in air, which is 344 m/s.\n",
      "\n",
      "The time taken by the sound wave in aluminium to reach the second child is t_aluminium = d / v_aluminium, where v_aluminium is the speed of sound in aluminium, which is 6420 m/s.\n",
      "\n",
      "Now, we need to find the ratio of these two times:\n",
      "\n",
      "t_air / t_aluminium = (d / v_air) / (d / v_aluminium) = v_aluminium / v_air = 6420 m/s / 344 m/s = 18.65\n",
      "\n",
      "So, the ratio of times taken by the sound wave in air and in aluminium to reach the second child is approximately 18.65.\n"
     ]
    }
   ],
   "source": [
    "query_engine, llm = create_rag_agent(index_markdown_element, llm)\n",
    "\n",
    "# Example usage\n",
    "ques = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "result = process_advanced_query(query_engine, llm, ques)\n",
    "print(f\"Query Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from typing import List\n",
    "from pydantic import Field\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from typing import Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional\n",
    "from llama_index.llms.groq import Groq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "#from llama_index.llm_predictor.base import BaseLLM\n",
    "\n",
    "#from llama_index.output_parsers.llm import LLMResult, Generation\n",
    "#from llama_index.callbacks.base import CallbackManagerForLLMRun\n",
    "\n",
    "# Wrapper class to adapt LlamaIndex VectorStoreIndex to LangChain BaseRetriever interface\n",
    "class LlamaIndexVectorStoreRetriever(BaseRetriever):\n",
    "    vector_store_index: VectorStoreIndex = Field(description=\"The LlamaIndex VectorStoreIndex\")\n",
    "    similarity_top_k: int = Field(default=5, description=\"Number of top similar documents to retrieve\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        retriever = self.vector_store_index.as_retriever(similarity_top_k=self.similarity_top_k)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        return [Document(page_content=node.node.get_content(), metadata=node.node.metadata) for node in nodes]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Implement async version if needed\n",
    "        return self._get_relevant_documents(query)\"\"\"\n",
    "\n",
    "\"\"\"class State:\n",
    "    #The state of the workflow.\n",
    "    def __init__(\n",
    "        self,\n",
    "        query: str,\n",
    "        initial_response: str,\n",
    "        additional_info: str = \"\",\n",
    "        final_answer: str = \"\",\n",
    "        needs_info: bool = False\n",
    "    ):\n",
    "        self.query = query\n",
    "        self.initial_response = initial_response\n",
    "        self.additional_info = additional_info\n",
    "        self.final_answer = final_answer\n",
    "        self.needs_info = needs_info\n",
    "\n",
    "class WorkflowState(BaseModel):\n",
    "    query: str\n",
    "    initial_response: str\n",
    "    additional_info: str = \"\"\n",
    "    final_answer: str = \"\"\n",
    "    needs_info: bool = False\n",
    "\n",
    "\n",
    "class GroqLLMWrapper(BaseLLM, BaseModel):\n",
    "    groq_llm: Groq = Field(..., description=\"The Groq LLM instance\")\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            response = self.groq_llm.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",  # Already passed in Groq instance\n",
    "                stop=stop,\n",
    "                **kwargs\n",
    "            )\n",
    "            generations.append([Generation(text=response.choices[0].message.content)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"groq\"\n",
    "\n",
    "    def predict_messages(self, messages: List[str], **kwargs) -> str:\n",
    "    # Convert HumanMessage objects to their text content\n",
    "        prompt = \" \".join([msg.content if isinstance(msg, HumanMessage) else msg for msg in messages])\n",
    "        response = self._generate([prompt], **kwargs)\n",
    "        return response.generations[0][0].text\"\"\"\n",
    "\n",
    "\n",
    "#def check_completeness(state: WorkflowState, llm: GroqLLMWrapper) -> Dict[str, WorkflowState]:\n",
    "#    prompt = ChatPromptTemplate.from_template(\n",
    "#        \"\"\"Given the following query and initial response, determine if the response contains \n",
    "\"\"\"        all necessary information to fully answer the query. If not, specify what additional \n",
    "        information is needed.\n",
    "\n",
    "        Query: {query}\n",
    "        Initial Response: {initial_response}\n",
    "\n",
    "        Output your response in the following format:\n",
    "        Complete: [Yes/No]\n",
    "        Additional Info Needed: [If No, specify what additional information is needed]\n",
    "        \"\"\"\n",
    "#    )\n",
    "    \n",
    "#    parser = StructuredOutputParser.from_response_schemas([\n",
    "#        ResponseSchema(name=\"Complete\", description=\"Whether the response is complete (Yes/No)\"),\n",
    "#        ResponseSchema(name=\"Additional Info Needed\", description=\"What additional information is needed, if any\")\n",
    "#    ])\n",
    "    \n",
    "#    response = llm.predict_messages(prompt.format_messages(\n",
    "#        query=state.query, \n",
    "#        initial_response=state.initial_response\n",
    "#    ))\n",
    "    \n",
    "#    parsed_response = parser.parse(response)\n",
    "#    state.needs_info = parsed_response[\"Complete\"].lower() == \"no\"\n",
    "#    if state.needs_info:\n",
    "#        state.additional_info = parsed_response[\"Additional Info Needed\"]\n",
    "    \n",
    "#    return {\"state\": state}\n",
    "\n",
    "\"\"\"def retrieve_additional_info(state: WorkflowState, retriever: LlamaIndexVectorStoreRetriever) -> Dict[str, WorkflowState]:\n",
    "    #Retrieve additional information if needed.\n",
    "    if state.needs_info:\n",
    "        additional_info_query = f\"Provide information about: {state.additional_info}\"\n",
    "        docs = retriever.get_relevant_documents(additional_info_query)\n",
    "        state.additional_info = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"state\": state}\"\"\"\n",
    "\n",
    "#def generate_final_answer(state: WorkflowState, llm: GroqLLMWrapper) -> Dict[str, WorkflowState]:\n",
    "#    prompt = ChatPromptTemplate.from_template(\n",
    "#        \"\"\"Given the following query, initial response, and additional information (if any),\n",
    "#        generate a complete and accurate answer.0\n",
    "\n",
    "#        Query: {query}\n",
    "#        Initial Response: {initial_response}\n",
    " #       Additional Information: {additional_info}\n",
    "\n",
    "#        Complete Answer:\n",
    "#        \"\"\"\n",
    "#    )\n",
    "    \n",
    "#    state.final_answer = llm.predict_messages(prompt.format_messages(\n",
    "#        query=state.query,\n",
    "#        initial_response=state.initial_response,\n",
    "#        additional_info=state.additional_info\n",
    "#    ))\n",
    "    \n",
    "#    return {\"state\": state}\n",
    "\n",
    "\"\"\"def create_workflow(llm: BaseLLM, retriever: LlamaIndexVectorStoreRetriever) -> StateGraph:\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"check_completeness\", lambda state: check_completeness(state, llm))\n",
    "    workflow.add_node(\"retrieve_additional_info\", lambda state: retrieve_additional_info(state, retriever))\n",
    "    workflow.add_node(\"generate_final_answer\", lambda state: generate_final_answer(state, llm))\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.add_edge(\"check_completeness\", \"retrieve_additional_info\")\n",
    "    workflow.add_edge(\"retrieve_additional_info\", \"generate_final_answer\")\n",
    "    workflow.add_edge(\"generate_final_answer\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"check_completeness\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "def solve_query_with_workflow(query: str, initial_response: str, groq_llm, vector_store_index: VectorStoreIndex) -> str:\n",
    "    # Pass groq_llm as a keyword argument\n",
    "    llm = GroqLLMWrapper(groq_llm=groq_llm)  \n",
    "    retriever_wrapper = LlamaIndexVectorStoreRetriever(vector_store_index=vector_store_index, similarity_top_k=5)\n",
    "    workflow = create_workflow(llm, retriever_wrapper)\n",
    "    \n",
    "    # Create the runnable graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Prepare the initial state\n",
    "    initial_state = WorkflowState(\n",
    "        query=query,\n",
    "        initial_response=initial_response\n",
    "    )\n",
    "    \n",
    "    # Run the workflow\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    return final_state.final_answer\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def index_setup(nodes, embed_model, db_path=\"./chroma_db\"):\n",
    "    # Create or connect to the persistent ChromaDB\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "    # Create ChromaVectorStore\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # Create storage context and index\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, storage_context=storage_context, embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    return index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def load_index(embed_model, db_path=\"./chroma_db\"):\n",
    "    # Load the index from persistent storage\n",
    "    db = chromadb.PersistentClient(path=db_path)\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    # Create the index from the vector store\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    return index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes_index = index_setup(nodes,embed_model, db_path=\"./chroma_db_nodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"query_engine = nodes_index.as_query_engine(llm=llm\n",
    "    ,similarity_top_k=3, verbose=True\n",
    ")\n",
    "\n",
    "question1 = \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\"\n",
    "\n",
    "response = query_engine.query(question1)\n",
    "print(response.source_nodes)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk , nodes \n",
    "\"\"\"db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_query_engine = index.as_query_engine(llm=llm\n",
    "    ,similarity_top_k=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is sound wave called a longitudinal wave?\"\n",
    "response = recursive_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[-2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[-2].text[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "1 : \"What is sound and how is it produced?\",\n",
    "\n",
    "2: \"Describe with the help of a diagram, how compressions and rarefactions are produced in air near a source of sound.\",\n",
    "\n",
    "3: \"Why is sound wave called a longitudinal wave?\",\n",
    "\n",
    "4: \"Which characteristic of the sound helps you to identify your friend by his voice while sitting with others in a dark room?\",\n",
    "\n",
    "5: \"Flash and thunder are produced simultaneously. But thunder is heard a few seconds after the flash is seen, why?\",\n",
    "\n",
    "6: \"A person has a hearing range from 20 Hz to 20 kHz. What are the typical wavelengths of sound waves in air corresponding to these two frequencies? Take the speed of sound in air as 344 m s⁻¹.\",\n",
    "\n",
    "7: \"Two children are at opposite ends of an aluminium rod. One strikes the end of the rod with a stone. Find the ratio of times taken by the sound wave in air and in aluminium to reach the second child.\",\n",
    "\n",
    "8: \"The frequency of a source of sound is 100 Hz. How many times does it vibrate in a minute?\",\n",
    "\n",
    "9: \"Does sound follow the same laws of reflection as light does? Explain.\",\n",
    "\n",
    "10: \"When a sound is reflected from a distant object, an echo is produced. Let the distance between the reflecting surface and the source of sound production remains the same. Do you hear echo sound on a hotter day?\",\n",
    "\n",
    "11: \"Give two practical applications of reflection of sound waves.\",\n",
    "\n",
    "12: \"A stone is dropped from the top of a tower 500 m high into a pond of water at the base of the tower. When is the splash heard at the top? Given, g = 10 m s⁻² and speed of sound = 340 m s⁻¹.\",\n",
    "\n",
    "13: \"A sound wave travels at a speed of 339 m s⁻¹. If its wavelength is 1.5 cm, what is the frequency of the wave? Will it be audible?\",\n",
    "\n",
    "14: \"What is reverberation? How can it be reduced?\",\n",
    "\n",
    "15: \"What is loudness of sound? What factors does it depend on?\",\n",
    "\n",
    "16: \"How is ultrasound used for cleaning?\",\n",
    "\n",
    "17: \"Explain how defects in a metal block can be detected using ultrasound.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for value in questions.values(): \n",
    "    repo = recursive_query_engine.query(value)\n",
    "    answers.append(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "for key, value in questions.items():\n",
    "    ans[key] = recursive_query_engine.query(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
